<%def name="render(x=0)">\
  notcompute:
    Metadata:
      OpenStack::Heat::Stack: {}
      Openstack::ImageBuilder::Elements:
      - boot-stack
      - heat-cfntools
      - heat-localip
      - neutron-network-node
      admin-password: unset
      admin-token: unset
      cinder:
        db: mysql://cinder:unset@localhost/cinder
        volume_size_mb: '5000'
      controller-address: 192.0.2.5
      db-password: unset
      glance:
        db: mysql://glance:unset@localhost/glance
        host: 192.0.2.5
      heat:
        access_key_id:
          Ref: Key
        admin_password: unset
        admin_tenant_name: service
        admin_user: heat
        auth_encryption_key: unset___________
        db: mysql://heat:unset@localhost/heat
        heat_watch_server_url: http://192.0.2.5:8003
        metadata_server_url: http://192.0.2.5:8000
        refresh:
        - resource: notcompute
        secret_key:
          Fn::GetAtt:
          - Key
          - SecretAccessKey
        stack:
          name:
            Ref: AWS::StackName
          region:
            Ref: AWS::Region
        waitcondition_server_url: http://192.0.2.5:8000/v1/waitcondition
      interfaces:
        control: eth2
      keystone:
        db: mysql://keystone:unset@localhost/keystone
        host: 192.0.2.5
      neutron:
        floatingip_end: 192.0.2.64
        floatingip_range: 192.0.2.0/24
        floatingip_start: 192.0.2.45
        host: 192.0.2.5
        metadata_proxy_shared_secret: unset
        ovs:
          enable_tunneling: 'True'
          fixed_range:
            end: 10.255.255.254
            start: 10.0.0.2
          local_ip: 192.0.2.5
          ovs_range: 10.0.0.0/8
          public_interface: eth0
          tenant_network_type: gre
        ovs_db: mysql://neutron:unset@localhost/ovs_neutron?charset=utf8
      nova:
        compute_driver: libvirt.LibvirtDriver
        db: mysql://nova:unset@localhost/nova
        host: 192.0.2.5
        metadata-proxy: true
      rabbit:
        host: 192.0.2.5
        password: guest
      service-password: unset
    Properties:
      ImageId:
        Ref: notcomputeImage
      InstanceType:
        Ref: InstanceType
      KeyName:
        Ref: KeyName
      AvailabilityZone: nova::${x}
      UserData:
        Fn::Base64:
          |
          #!/bin/bash -v
          cat >/root/tuskar-provision.sh <<EOL
          #!/bin/bash -v
          /opt/aws/bin/cfn-init
          #0. SOURCE CREDENTIALS FOR NOVA COMMANDS
          source /root/stackrc
          wait_for(){
            LOOPS=\$1
            SLEEPTIME=\$2
            shift ; shift
            i=0
            while [ \$i -lt \$LOOPS ] ; do
                i=\$((i + 1))
                eval "\$@" && return 0 || true
                sleep \$SLEEPTIME
            done
            return 1
          }
          wait_for 60 10 test -f /opt/stack/boot-stack.ok
          wait_for 60 10 nova list
          % for rc in resource_classes:
          #1. CREAT HOST AGGREGATE for each resource class
          AGGREGATE_ID=\$(nova aggregate-create "${rc.name}-hosts" | tail -n +4 | head -n 1 | tr -s ' ' | cut -d '|' -f2)
          #2. SET HOST AGGREGATE METADATA
          nova aggregate-set-metadata \$AGGREGATE_ID class="${rc.name}"
          #3. CREATE EACH FLAVOR
        % for f in rc.flavors:
          nova flavor-show "${rc.name}.${f.name}" &> /dev/null
          if [ \$? == 1 ]; then
          <%
            ram, vcpu, disk, ephemeral, swap = nova_util.extract_from_capacities(f)
          %>
            nova flavor-create --ephemeral=${ephemeral} --swap=${swap} "${rc.name}.${f.name}" "auto" ${ram} ${disk} ${vcpu}
          fi
          #4. SET FLAVOR METADATA
          nova flavor-key "${rc.name}.${f.name}" set class="${rc.name}"
        % endfor
          #5. ADD NODES TO HOST AGGREGATE
      % for rack in rc.racks:
        % for node in rack.nodes:
          nova aggregate-add-host \$AGGREGATE_ID ${node.node_id}
        % endfor
        % endfor
        % endfor
          EOL
          chmod +x /root/tuskar-provision.sh
          exec /root/tuskar-provision.sh &> /root/tuskar-provision.log &
    Type: AWS::EC2::Instance
</%def>\
